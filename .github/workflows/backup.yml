# å¤‡ä»½å·¥ä½œæµï¼ˆé»˜è®¤ç¦ç”¨ï¼‰
name: Backup

on:
  # é»˜è®¤ç¦ç”¨è‡ªåŠ¨è§¦å‘ï¼Œéœ€è¦æ‰‹åŠ¨å¯ç”¨
  # schedule:
  #   # æ¯å¤©å‡Œæ™¨ 4:00 UTC æ‰§è¡Œå¤‡ä»½
  #   - cron: "0 4 * * *"
  #   # æ¯å‘¨æ—¥å‡Œæ™¨ 2:00 UTC æ‰§è¡Œå®Œæ•´å¤‡ä»½
  #   - cron: "0 2 * * 0"
  workflow_dispatch:
    inputs:
      enable_workflow:
        description: "å¯ç”¨å·¥ä½œæµï¼ˆé»˜è®¤ç¦ç”¨ï¼‰"
        required: true
        default: false
        type: boolean
      backup_type:
        description: "å¤‡ä»½ç±»å‹"
        required: true
        default: "incremental"
        type: choice
        options:
          - incremental
          - full
      retention_days:
        description: "ä¿ç•™å¤©æ•°"
        required: false
        default: "30"
        type: string

env:
  AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}
  BACKUP_BUCKET: ${{ secrets.BACKUP_BUCKET }}
  BACKUP_TYPE: ${{ inputs.backup_type || 'incremental' }}
  RETENTION_DAYS: ${{ inputs.retention_days || '30' }}

jobs:
  # ===== æ•°æ®åº“å¤‡ä»½ =====
  database-backup:
    name: æ•°æ®åº“å¤‡ä»½
    runs-on: ubuntu-latest
    if: ${{ inputs.enable_workflow == true && env.BACKUP_BUCKET != '' }}
    outputs:
      backup-file: ${{ steps.backup.outputs.backup-file }}

    steps:
      - name: æ£€å‡ºä»£ç 
        uses: actions/checkout@v4

      - name: è®¾ç½®å¤‡ä»½ç¯å¢ƒ
        run: |
          # åˆ›å»ºå¤‡ä»½ç›®å½•
          mkdir -p backups/$(date +%Y%m%d)

          # è®¾ç½®å¤‡ä»½æ–‡ä»¶å
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          BACKUP_FILE="domain-max-db-${BACKUP_TYPE}-${TIMESTAMP}.sql.gz"
          echo "BACKUP_FILE=$BACKUP_FILE" >> $GITHUB_ENV

      - name: å¯åŠ¨æ•°æ®åº“æœåŠ¡
        run: |
          # ä½¿ç”¨æµ‹è¯•é…ç½®å¯åŠ¨æ•°æ®åº“
          cd deployments
          cp .env.example .env
          echo "DB_PASSWORD=backup123" >> .env
          echo "JWT_SECRET=backup-test-secret" >> .env
          echo "ENCRYPTION_KEY=12345678901234567890123456789012" >> .env

          # åªå¯åŠ¨æ•°æ®åº“æœåŠ¡
          docker-compose up -d postgres

          # ç­‰å¾…æ•°æ®åº“å¯åŠ¨
          timeout 60 bash -c 'until docker-compose exec -T postgres pg_isready -U postgres; do sleep 5; done'

      - name: æ‰§è¡Œæ•°æ®åº“å¤‡ä»½
        id: backup
        run: |
          cd deployments

          if [ "$BACKUP_TYPE" = "full" ]; then
            echo "æ‰§è¡Œå®Œæ•´æ•°æ®åº“å¤‡ä»½..."
            docker-compose exec -T postgres pg_dumpall -U postgres | gzip > "../backups/$(date +%Y%m%d)/$BACKUP_FILE"
          else
            echo "æ‰§è¡Œå¢é‡æ•°æ®åº“å¤‡ä»½..."
            docker-compose exec -T postgres pg_dump -U postgres domain_manager | gzip > "../backups/$(date +%Y%m%d)/$BACKUP_FILE"
          fi

          # éªŒè¯å¤‡ä»½æ–‡ä»¶
          if [ -f "../backups/$(date +%Y%m%d)/$BACKUP_FILE" ]; then
            BACKUP_SIZE=$(stat -c%s "../backups/$(date +%Y%m%d)/$BACKUP_FILE")
            echo "å¤‡ä»½æ–‡ä»¶å¤§å°: $BACKUP_SIZE bytes"
            
            if [ $BACKUP_SIZE -lt 1000 ]; then
              echo "é”™è¯¯: å¤‡ä»½æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½å¤‡ä»½å¤±è´¥"
              exit 1
            fi
            
            echo "backup-file=backups/$(date +%Y%m%d)/$BACKUP_FILE" >> $GITHUB_OUTPUT
          else
            echo "é”™è¯¯: å¤‡ä»½æ–‡ä»¶æœªåˆ›å»º"
            exit 1
          fi

      - name: ä¸Šä¼ å¤‡ä»½åˆ° GitHub Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: database-backup-${{ env.BACKUP_TYPE }}
          path: backups/
          retention-days: 7

      - name: é…ç½® AWS CLI
        if: env.BACKUP_BUCKET != ''
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: ä¸Šä¼ å¤‡ä»½åˆ° S3
        if: env.BACKUP_BUCKET != ''
        run: |
          BACKUP_PATH="backups/$(date +%Y%m%d)/$BACKUP_FILE"
          S3_PATH="s3://$BACKUP_BUCKET/database/$(date +%Y)/$(date +%m)/$BACKUP_FILE"

          echo "ä¸Šä¼ å¤‡ä»½åˆ° S3: $S3_PATH"
          aws s3 cp "$BACKUP_PATH" "$S3_PATH"

          # è®¾ç½®å¤‡ä»½æ ‡ç­¾
          aws s3api put-object-tagging \
            --bucket "$BACKUP_BUCKET" \
            --key "database/$(date +%Y)/$(date +%m)/$BACKUP_FILE" \
            --tagging "TagSet=[{Key=BackupType,Value=$BACKUP_TYPE},{Key=Project,Value=domain-max},{Key=Date,Value=$(date +%Y%m%d)}]"

      - name: æ¸…ç†æœ¬åœ°å¤‡ä»½
        run: |
          rm -rf backups/

      - name: åœæ­¢æ•°æ®åº“æœåŠ¡
        if: always()
        run: |
          cd deployments
          docker-compose down -v

  # ===== é…ç½®æ–‡ä»¶å¤‡ä»½ =====
  config-backup:
    name: é…ç½®æ–‡ä»¶å¤‡ä»½
    runs-on: ubuntu-latest
    outputs:
      config-backup-file: ${{ steps.backup.outputs.config-backup-file }}

    steps:
      - name: æ£€å‡ºä»£ç 
        uses: actions/checkout@v4

      - name: åˆ›å»ºé…ç½®å¤‡ä»½
        id: backup
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          CONFIG_BACKUP_FILE="domain-max-config-${TIMESTAMP}.tar.gz"

          # å¤‡ä»½é…ç½®æ–‡ä»¶å’Œè„šæœ¬
          tar -czf "$CONFIG_BACKUP_FILE" \
            --exclude='.git' \
            --exclude='node_modules' \
            --exclude='*.log' \
            --exclude='.env' \
            deployments/ \
            scripts/ \
            docs/ \
            configs/ \
            README.md \
            LICENSE \
            Makefile

          echo "config-backup-file=$CONFIG_BACKUP_FILE" >> $GITHUB_OUTPUT

          # éªŒè¯å¤‡ä»½æ–‡ä»¶
          if [ -f "$CONFIG_BACKUP_FILE" ]; then
            echo "é…ç½®å¤‡ä»½æ–‡ä»¶åˆ›å»ºæˆåŠŸ: $CONFIG_BACKUP_FILE"
            ls -lh "$CONFIG_BACKUP_FILE"
          else
            echo "é”™è¯¯: é…ç½®å¤‡ä»½æ–‡ä»¶åˆ›å»ºå¤±è´¥"
            exit 1
          fi

      - name: ä¸Šä¼ é…ç½®å¤‡ä»½åˆ° GitHub Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: config-backup
          path: ${{ steps.backup.outputs.config-backup-file }}
          retention-days: 30

      - name: é…ç½® AWS CLI
        if: env.BACKUP_BUCKET != ''
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: ä¸Šä¼ é…ç½®å¤‡ä»½åˆ° S3
        if: env.BACKUP_BUCKET != ''
        run: |
          CONFIG_FILE="${{ steps.backup.outputs.config-backup-file }}"
          S3_PATH="s3://$BACKUP_BUCKET/configs/$(date +%Y)/$(date +%m)/$CONFIG_FILE"

          echo "ä¸Šä¼ é…ç½®å¤‡ä»½åˆ° S3: $S3_PATH"
          aws s3 cp "$CONFIG_FILE" "$S3_PATH"

          # è®¾ç½®å¤‡ä»½æ ‡ç­¾
          aws s3api put-object-tagging \
            --bucket "$BACKUP_BUCKET" \
            --key "configs/$(date +%Y)/$(date +%m)/$CONFIG_FILE" \
            --tagging "TagSet=[{Key=BackupType,Value=config},{Key=Project,Value=domain-max},{Key=Date,Value=$(date +%Y%m%d)}]"

  # ===== å®¹å™¨é•œåƒå¤‡ä»½ =====
  image-backup:
    name: å®¹å™¨é•œåƒå¤‡ä»½
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' && github.event.schedule == '0 2 * * 0'

    steps:
      - name: æ£€å‡ºä»£ç 
        uses: actions/checkout@v4

      - name: è®¾ç½® Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: ç™»å½•åˆ°å®¹å™¨æ³¨å†Œè¡¨
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: æ„å»ºå¹¶æ¨é€å¤‡ä»½é•œåƒ
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./deployments/Dockerfile
          platforms: linux/amd64,linux/arm64
          push: true
          tags: |
            ghcr.io/${{ github.repository }}:backup-$(date +%Y%m%d)
            ghcr.io/${{ github.repository }}:backup-latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: å¯¼å‡ºé•œåƒåˆ°æ–‡ä»¶
        if: env.BACKUP_BUCKET != ''
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          IMAGE_BACKUP_FILE="domain-max-image-${TIMESTAMP}.tar.gz"

          # å¯¼å‡ºé•œåƒ
          docker save ghcr.io/${{ github.repository }}:backup-latest | gzip > "$IMAGE_BACKUP_FILE"

          echo "é•œåƒå¤‡ä»½æ–‡ä»¶: $IMAGE_BACKUP_FILE"
          ls -lh "$IMAGE_BACKUP_FILE"

      - name: é…ç½® AWS CLI
        if: env.BACKUP_BUCKET != ''
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: ä¸Šä¼ é•œåƒå¤‡ä»½åˆ° S3
        if: env.BACKUP_BUCKET != ''
        run: |
          IMAGE_FILE="domain-max-image-$(date +%Y%m%d_%H%M%S).tar.gz"
          S3_PATH="s3://$BACKUP_BUCKET/images/$(date +%Y)/$(date +%m)/$IMAGE_FILE"

          echo "ä¸Šä¼ é•œåƒå¤‡ä»½åˆ° S3: $S3_PATH"
          aws s3 cp "$IMAGE_FILE" "$S3_PATH"

  # ===== æ¸…ç†æ—§å¤‡ä»½ =====
  cleanup-old-backups:
    name: æ¸…ç†æ—§å¤‡ä»½
    runs-on: ubuntu-latest
    needs: [database-backup, config-backup]
    if: always() && env.BACKUP_BUCKET != ''

    steps:
      - name: é…ç½® AWS CLI
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: æ¸…ç†è¿‡æœŸçš„æ•°æ®åº“å¤‡ä»½
        run: |
          echo "æ¸…ç† $RETENTION_DAYS å¤©å‰çš„æ•°æ®åº“å¤‡ä»½..."

          # åˆ—å‡ºè¶…è¿‡ä¿ç•™æœŸçš„å¤‡ä»½æ–‡ä»¶
          CUTOFF_DATE=$(date -d "$RETENTION_DAYS days ago" +%Y-%m-%d)

          aws s3api list-objects-v2 \
            --bucket "$BACKUP_BUCKET" \
            --prefix "database/" \
            --query "Contents[?LastModified<='$CUTOFF_DATE'].Key" \
            --output text | while read -r key; do
              if [ -n "$key" ] && [ "$key" != "None" ]; then
                echo "åˆ é™¤è¿‡æœŸå¤‡ä»½: $key"
                aws s3 rm "s3://$BACKUP_BUCKET/$key"
              fi
            done

      - name: æ¸…ç†è¿‡æœŸçš„é…ç½®å¤‡ä»½
        run: |
          echo "æ¸…ç† $RETENTION_DAYS å¤©å‰çš„é…ç½®å¤‡ä»½..."

          CUTOFF_DATE=$(date -d "$RETENTION_DAYS days ago" +%Y-%m-%d)

          aws s3api list-objects-v2 \
            --bucket "$BACKUP_BUCKET" \
            --prefix "configs/" \
            --query "Contents[?LastModified<='$CUTOFF_DATE'].Key" \
            --output text | while read -r key; do
              if [ -n "$key" ] && [ "$key" != "None" ]; then
                echo "åˆ é™¤è¿‡æœŸé…ç½®å¤‡ä»½: $key"
                aws s3 rm "s3://$BACKUP_BUCKET/$key"
              fi
            done

      - name: æ¸…ç†è¿‡æœŸçš„é•œåƒå¤‡ä»½
        run: |
          echo "æ¸…ç† 90 å¤©å‰çš„é•œåƒå¤‡ä»½..."

          CUTOFF_DATE=$(date -d "90 days ago" +%Y-%m-%d)

          aws s3api list-objects-v2 \
            --bucket "$BACKUP_BUCKET" \
            --prefix "images/" \
            --query "Contents[?LastModified<='$CUTOFF_DATE'].Key" \
            --output text | while read -r key; do
              if [ -n "$key" ] && [ "$key" != "None" ]; then
                echo "åˆ é™¤è¿‡æœŸé•œåƒå¤‡ä»½: $key"
                aws s3 rm "s3://$BACKUP_BUCKET/$key"
              fi
            done

  # ===== å¤‡ä»½éªŒè¯ =====
  verify-backup:
    name: éªŒè¯å¤‡ä»½
    runs-on: ubuntu-latest
    needs: [database-backup, config-backup]
    if: always() && needs.database-backup.result == 'success'

    steps:
      - name: æ£€å‡ºä»£ç 
        uses: actions/checkout@v4

      - name: ä¸‹è½½æ•°æ®åº“å¤‡ä»½
        uses: actions/download-artifact@v3
        with:
          name: database-backup-${{ env.BACKUP_TYPE }}
          path: backup-verification/

      - name: éªŒè¯æ•°æ®åº“å¤‡ä»½
        run: |
          cd backup-verification

          # æŸ¥æ‰¾å¤‡ä»½æ–‡ä»¶
          BACKUP_FILE=$(find . -name "*.sql.gz" | head -1)

          if [ -n "$BACKUP_FILE" ]; then
            echo "æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶: $BACKUP_FILE"
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            SIZE=$(stat -c%s "$BACKUP_FILE")
            echo "å¤‡ä»½æ–‡ä»¶å¤§å°: $SIZE bytes"
            
            if [ $SIZE -lt 1000 ]; then
              echo "é”™è¯¯: å¤‡ä»½æ–‡ä»¶å¤ªå°"
              exit 1
            fi
            
            # æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
            if zcat "$BACKUP_FILE" | head -10 | grep -q "PostgreSQL"; then
              echo "âœ… å¤‡ä»½æ–‡ä»¶éªŒè¯é€šè¿‡"
            else
              echo "âŒ å¤‡ä»½æ–‡ä»¶éªŒè¯å¤±è´¥"
              exit 1
            fi
          else
            echo "âŒ æœªæ‰¾åˆ°å¤‡ä»½æ–‡ä»¶"
            exit 1
          fi

      - name: æµ‹è¯•å¤‡ä»½æ¢å¤
        run: |
          cd deployments

          # å¯åŠ¨æµ‹è¯•æ•°æ®åº“
          cp .env.example .env
          echo "DB_PASSWORD=restore_test123" >> .env
          echo "JWT_SECRET=restore-test-secret" >> .env
          echo "ENCRYPTION_KEY=12345678901234567890123456789012" >> .env

          docker-compose up -d postgres
          timeout 60 bash -c 'until docker-compose exec -T postgres pg_isready -U postgres; do sleep 5; done'

          # å°è¯•æ¢å¤å¤‡ä»½
          BACKUP_FILE=$(find ../backup-verification -name "*.sql.gz" | head -1)

          if [ -n "$BACKUP_FILE" ]; then
            echo "æµ‹è¯•æ¢å¤å¤‡ä»½æ–‡ä»¶: $BACKUP_FILE"
            zcat "$BACKUP_FILE" | docker-compose exec -T postgres psql -U postgres -d domain_manager
            
            # éªŒè¯æ¢å¤ç»“æœ
            TABLES=$(docker-compose exec -T postgres psql -U postgres -d domain_manager -c "\dt" | grep -c "table")
            echo "æ¢å¤åæ•°æ®åº“è¡¨æ•°é‡: $TABLES"
            
            if [ $TABLES -gt 0 ]; then
              echo "âœ… å¤‡ä»½æ¢å¤æµ‹è¯•é€šè¿‡"
            else
              echo "âŒ å¤‡ä»½æ¢å¤æµ‹è¯•å¤±è´¥"
              exit 1
            fi
          fi

          # æ¸…ç†æµ‹è¯•ç¯å¢ƒ
          docker-compose down -v

  # ===== ç”Ÿæˆå¤‡ä»½æŠ¥å‘Š =====
  backup-report:
    name: ç”Ÿæˆå¤‡ä»½æŠ¥å‘Š
    runs-on: ubuntu-latest
    needs: [database-backup, config-backup, image-backup, verify-backup]
    if: always()

    steps:
      - name: ç”Ÿæˆå¤‡ä»½æŠ¥å‘Š
        run: |
          echo "# ğŸ“¦ Domain MAX å¤‡ä»½æŠ¥å‘Š" > backup-report.md
          echo "" >> backup-report.md
          echo "**å¤‡ä»½æ—¶é—´**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> backup-report.md
          echo "**å¤‡ä»½ç±»å‹**: $BACKUP_TYPE" >> backup-report.md
          echo "**ä¿ç•™æœŸé™**: $RETENTION_DAYS å¤©" >> backup-report.md
          echo "" >> backup-report.md

          echo "## ğŸ“‹ å¤‡ä»½çŠ¶æ€" >> backup-report.md
          echo "" >> backup-report.md
          echo "| å¤‡ä»½é¡¹ç›® | çŠ¶æ€ | è¯´æ˜ |" >> backup-report.md
          echo "|---------|------|------|" >> backup-report.md
          echo "| æ•°æ®åº“å¤‡ä»½ | ${{ needs.database-backup.result == 'success' && 'âœ… æˆåŠŸ' || 'âŒ å¤±è´¥' }} | PostgreSQL æ•°æ®åº“å¤‡ä»½ |" >> backup-report.md
          echo "| é…ç½®å¤‡ä»½ | ${{ needs.config-backup.result == 'success' && 'âœ… æˆåŠŸ' || 'âŒ å¤±è´¥' }} | é…ç½®æ–‡ä»¶å’Œè„šæœ¬å¤‡ä»½ |" >> backup-report.md
          echo "| é•œåƒå¤‡ä»½ | ${{ needs.image-backup.result == 'success' && 'âœ… æˆåŠŸ' || needs.image-backup.result == 'skipped' && 'â­ï¸ è·³è¿‡' || 'âŒ å¤±è´¥' }} | Docker é•œåƒå¤‡ä»½ |" >> backup-report.md
          echo "| å¤‡ä»½éªŒè¯ | ${{ needs.verify-backup.result == 'success' && 'âœ… é€šè¿‡' || 'âŒ å¤±è´¥' }} | å¤‡ä»½æ–‡ä»¶å®Œæ•´æ€§éªŒè¯ |" >> backup-report.md
          echo "" >> backup-report.md

          echo "## ğŸ“Š å¤‡ä»½ç»Ÿè®¡" >> backup-report.md
          echo "" >> backup-report.md
          echo "- **æ•°æ®åº“å¤‡ä»½**: ${{ needs.database-backup.outputs.backup-file || 'æœªç”Ÿæˆ' }}" >> backup-report.md
          echo "- **é…ç½®å¤‡ä»½**: ${{ needs.config-backup.outputs.config-backup-file || 'æœªç”Ÿæˆ' }}" >> backup-report.md
          echo "- **å­˜å‚¨ä½ç½®**: ${{ env.BACKUP_BUCKET && 'AWS S3' || 'GitHub Artifacts' }}" >> backup-report.md
          echo "" >> backup-report.md

          echo "## ğŸ”„ æ¢å¤è¯´æ˜" >> backup-report.md
          echo "" >> backup-report.md
          echo "å¦‚éœ€æ¢å¤å¤‡ä»½ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š" >> backup-report.md
          echo "" >> backup-report.md
          echo "### æ•°æ®åº“æ¢å¤" >> backup-report.md
          echo "\`\`\`bash" >> backup-report.md
          echo "# ä¸‹è½½å¤‡ä»½æ–‡ä»¶" >> backup-report.md
          echo "aws s3 cp s3://$BACKUP_BUCKET/database/YYYY/MM/backup-file.sql.gz ." >> backup-report.md
          echo "" >> backup-report.md
          echo "# æ¢å¤æ•°æ®åº“" >> backup-report.md
          echo "zcat backup-file.sql.gz | docker-compose exec -T postgres psql -U postgres -d domain_manager" >> backup-report.md
          echo "\`\`\`" >> backup-report.md
          echo "" >> backup-report.md
          echo "### é…ç½®æ¢å¤" >> backup-report.md
          echo "\`\`\`bash" >> backup-report.md
          echo "# ä¸‹è½½é…ç½®å¤‡ä»½" >> backup-report.md
          echo "aws s3 cp s3://$BACKUP_BUCKET/configs/YYYY/MM/config-file.tar.gz ." >> backup-report.md
          echo "" >> backup-report.md
          echo "# è§£å‹é…ç½®æ–‡ä»¶" >> backup-report.md
          echo "tar -xzf config-file.tar.gz" >> backup-report.md
          echo "\`\`\`" >> backup-report.md

      - name: ä¸Šä¼ å¤‡ä»½æŠ¥å‘Š
        uses: actions/upload-artifact@v3
        with:
          name: backup-report
          path: backup-report.md

      - name: å‘é€å¤‡ä»½é€šçŸ¥
        if: github.event_name == 'schedule' && secrets.SLACK_WEBHOOK_URL
        uses: 8398a7/action-slack@v3
        with:
          status: custom
          custom_payload: |
            {
              "text": "ğŸ“¦ Domain MAX å¤‡ä»½ä»»åŠ¡å®Œæˆ",
              "attachments": [
                {
                  "color": "${{ contains(needs.*.result, 'failure') && 'danger' || 'good' }}",
                  "fields": [
                    {
                      "title": "å¤‡ä»½ç±»å‹",
                      "value": "${{ env.BACKUP_TYPE }}",
                      "short": true
                    },
                    {
                      "title": "å¤‡ä»½çŠ¶æ€",
                      "value": "${{ contains(needs.*.result, 'failure') && 'éƒ¨åˆ†å¤±è´¥' || 'å…¨éƒ¨æˆåŠŸ' }}",
                      "short": true
                    },
                    {
                      "title": "æ•°æ®åº“å¤‡ä»½",
                      "value": "${{ needs.database-backup.result }}",
                      "short": true
                    },
                    {
                      "title": "é…ç½®å¤‡ä»½",
                      "value": "${{ needs.config-backup.result }}",
                      "short": true
                    }
                  ]
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
